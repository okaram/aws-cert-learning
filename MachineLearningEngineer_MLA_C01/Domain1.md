# Domain 1:  Data Preparation for Machine Learning (ML) (28%

## Task Statement 1.1: Ingest and store data.

Knowledge of:
***

### Data formats and ingestion mechanisms (for example, validated and non-validated formats)

SageMaker supports many different formats for [Training](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html) and [Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-inference.html), including:

* [Apache Parquet](https://parquet.apache.org/) - an open source, column-oriented data file format designed for efficient data storage and retrieval.
* [JSON](https://www.json.org/) - Javascript Object Notation, a text-based format close to how Javascript would represent a dictionary or object.
* [CSV](https://en.wikipedia.org/wiki/Comma-separated_values) - Comma-separated values. Text based, each line is a row, values are separated by commas; may wrap values in "" if they contain commas; may contain a header or not.
* [Apache ORC](https://orc.apache.org/) - Another columnar data format; binary, includes indexes for each column, supports complex types.
* [Apache Avro](https://avro.apache.org/) - serialization format for record data.
* [RecordIO](https://mesos.apache.org/documentation/latest/recordio/) - Binary, data is a series of records, for each record we first put its lengh in bytes, and then the bytes, which are usually binary, may be compressed.

###  How to use the core AWS data sources (for example, Amazon S3, Amazon Elastic File System [Amazon EFS], Amazon FSx for NetApp ONTAP)

### How to use AWS streaming data sources to ingest data (for example, Amazon Kinesis, Apache Flink, Apache Kafka)

### AWS storage options, including use cases and tradeoffs

* Amazon S3
* EBS
* EFS
* FSx (several versions)
* Storage gateway
* Databases
    * DynamoDB
    * RDS
    * Aurora
    * Timestream
    * ...
***
Skills in:
***

### Extracting data from storage (for example, Amazon S3, Amazon ElasticBlock Store [Amazon EBS], Amazon EFS, Amazon RDS, Amazon DynamoDB) by using relevant AWS service options (for example, Amazon S3 Transfer Acceleration, Amazon EBS Provisioned IOPS)

Amazon S3 is a *scalable* object storage system. 

Amazon EBS provides a low-level local-network block storage. You can create EBS volumes and attach them to one or a few EC2 instances.

Amazon EFS provides a scalable NFS file system over the network (usually, a local network). EFS file systems can be accessed by many EC2 instances over the network. FSx provides several different file systems, FSx for NetApp ONTAP provides NSF as well as SMB and CIFS, and FSx for Lustre provides a *very* high performance parallel file system.

Amazon RDS provides relational database clusters, either PostgreSQL or MySQL. Amazon Aurora also provides either of those DBs, with a different storage layers.

DynamoDB is a highly scalable key-value store, where the values can be complex objects. Since it also supports global indexes, you can provide flexible access mechanisms, but it's harder than a relational db.

### Choosing appropriate data formats (for example, Parquet, JSON, CSV, ORC) based on data access patterns

Text based formats, like JSON or CSV are human readable, and usually easy to generate without external libraries, but require extra storage and are slow, since numeric data has to be parsed. Binary formats are harder to generate and require special libraries, but are smaller and faster to process.




### Ingesting data into Amazon SageMaker Data Wrangler and SageMaker Feature Store

### Merging data from multiple sources (for example, by using programming techniques, AWS Glue, Apache Spark)

### Troubleshooting and debugging data ingestion and storage issues that involve capacity and scalability

### Making initial storage decisions based on cost, performance, and data structure

## Task Statement 1.2: Transform data and perform feature engineering.

Knowledge of:
***

### Data cleaning and transformation techniques (for example, detecting and treating outliers, imputing missing data, combining, deduplication)

An **outlier** is a data point that is way outside the range of the other data points in a set, so much that we think it may have been generated by a different process.

Sagemaker AI provides the [Random Cut Forest](https://docs.aws.amazon.com/sagemaker/latest/dg/algorithms-unsupervised.html) algorithm specifically for anomaly/outlier detection, besides statistical algorithms, like z-score, or simple exploration.

Once we identify outliers, we can, in general, either remove them from the dataset, replace them with an estimate, or reduce their weight.


### Feature engineering techniques (for example, data scaling and standardization, feature splitting, binning, log transformation, normalization)
### Encoding techniques (for example, one-hot encoding, binary encoding, label encoding, tokenization)
### Tools to explore, visualize, or transform data and features (for example, SageMaker Data Wrangler, AWS Glue, AWS Glue DataBrew)
### Services that transform streaming data (for example, AWS Lambda, Spark)
### Data annotation and labeling services that create high-quality labeled datasets

Skills in:
***

### Transforming data by using AWS tools (for example, AWS Glue, AWS Glue DataBrew, Spark running on Amazon EMR, SageMaker Data Wrangler)

### Creating and managing features by using AWS tools (for example, SageMaker Feature Store)

### Validating and labeling data by using AWS services (for example, SageMaker Ground Truth, Amazon Mechanical Turk)

## Task Statement 1.3: Ensure data integrity and prepare data for modeling.
***
Knowledge of:
***

* Pre-training bias metrics for numeric, text, and image data (for example,
class imbalance [CI], difference in proportions of labels [DPL])
* Strategies to address CI in numeric, text, and image datasets (for example,
synthetic data generation, resampling)
* Techniques to encrypt data
* Data classification, anonymization, and masking
* Implications of compliance requirements (for example, personally
identifiable information [PII], protected health information [PHI], data
residency) 

***
Skills in:
***

* Validating data quality (for example, by using AWS Glue DataBrew and AWS
Glue Data Quality)
* Identifying and mitigating sources of bias in data (for example, selection
bias, measurement bias) by using AWS tools (for example, SageMaker
Clarify)
* Preparing data to reduce prediction bias (for example, by using dataset
splitting, shuffling, and augmentation)
* Configuring data to load into the model training resource (for example,
Amazon EFS, Amazon FSx)
